{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7151f6",
   "metadata": {},
   "source": [
    "### PyTorch によるモデル作成の手順\n",
    "#### 1. 必要なものを定義\n",
    "- Dataset, transform, DataLoader\n",
    "- Model\n",
    "- Loss\n",
    "- Optimizer\n",
    "- Scheduler\n",
    "- device\n",
    "\n",
    "#### 2. 訓練フローの構築\n",
    "- 訓練フロー\n",
    "- 検証フロー\n",
    "\n",
    "#### 3. 実行\n",
    "- モデルの保存\n",
    "- 実験管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52cd01",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-1-1. Dataset の定義\n",
    "この3パターンのどれかで取得する\n",
    "1. 公式からimport\n",
    "2. ImageFolderを使う\n",
    "3. 自作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b37d3",
   "metadata": {},
   "source": [
    "1. 公式からimport\n",
    "\n",
    "    PyTorch の torchvisionには、代表的な画像データセット（MNIST、CIFAR-10、ImageNet など）が用意されており、簡単に利用できる．\n",
    "    - 8-9割はこれ．\n",
    "    - root は dataset が属するディレクトリ. dataset 自体のパスじゃない\n",
    "    - download=True で，パスになかったら DL してくれる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ee115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root = \"path/to/datasets\", train=True, download=True)\n",
    "val_ds = torchvision.datasets.CIFAR10(root = \"path/to/datasets\", train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ffff9",
   "metadata": {},
   "source": [
    "2. ImageFolder を使う\n",
    "\n",
    "    こんな構造のディレクトリのパスを渡すと，    \n",
    "    自動でクラスを割り当て，画像を読み取ってくれる．\n",
    "\n",
    "    <img src=\"imagefolder.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_ds = ImageFolder(root=\"path/to/animal_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60cdb51",
   "metadata": {},
   "source": [
    "3. 自作\n",
    "\n",
    "    1, 2 が使えないならオリジナルの Datasetクラス を自作するのが基本．\n",
    "\n",
    "    必須要件\n",
    "\n",
    "    - `__len__(self)`    \n",
    "        データセットの総サンプル数を返す関数が必要．len(dataset) が機能するように\n",
    "\n",
    "    - `__getitem__(self, idx)`\n",
    "    \n",
    "        指定したインデックスのサンプルを返す必要がある．        \n",
    "        dataset[0] や dataset[200] のようにデータにアクセスしたり，`for`文でイテレートできるように\n",
    "\n",
    "    推奨事項\n",
    "    - `transform` や `target_transform` を `__init__` で受け取り,`__getitem__` 内でデータに適用させること\n",
    "    - 画像データセットとして定義する場合，`__getitem__` は，画像（`PIL.Image.Image`）と正解ラベル（`int`）の **タプル** を返すこと．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムデータセットの例\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # RGBに変換\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4691feb",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-1-2. transform の定義\n",
    "- データに対して行いたい前処理がある場合，`Dataset`の`transform`や`target_transform`に前処理を定義することができる．\n",
    "\n",
    "- `torchvision.transforms` を用い、`Compose` で複数の操作をまとめて定義する．\n",
    "\n",
    "- 画像データに対しては，`PIL.Image.Image`型のデータを，平均0, 分散1の`Tensor`に `Normalize` したうえで，必要に応じてデータ拡張を行う．\n",
    "\n",
    "- `Normalize` の引数は使うデータセットによって変わる．調べて適切な値を入れる必要がある．\n",
    "\n",
    "- 検証用データセットにデータ拡張かけないように\n",
    "    - `train_ds`: `Data Augmentation` + `ToTensor` + `Normalize`\n",
    "    - `val_ds`: `ToTensor` + `Normalize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform の例\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                           # 入力サイズを統一\n",
    "    transforms.RandomHorizontalFlip(p=0.5),                  # 水平反転によるデータ拡張\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),   # 明るさ・コントラストをランダムに変化\n",
    "    transforms.ToTensor(),                                   # PIL→Tensor（値を [0,1] にスケーリング）\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],         # ImageNet 標準の平均\n",
    "                         std=[0.229, 0.224, 0.225])         # ImageNet 標準の分散\n",
    "])\n",
    "\n",
    "# データセットに適用\n",
    "train_ds = torchvision.datasets.CIFAR10(root = \"path/to/datasets\", train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699789d3",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-1-3. DataLoader の定義\n",
    "- 作った`Dataset`を`DataLoader`の引数に渡すことで，ミニバッチを作ったり，ミニバッチの内容をシャッフルするのに必要．\n",
    "\n",
    "- 引数を変えるだけで高速化．とりあえず書いとけってのがある．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader の例\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,               # Dataset オブジェクト\n",
    "    batch_size=batch_size,         # ミニバッチのサイズ\n",
    "    shuffle=True,          # エポックごとにシャッフル（デフォルトでTrue）\n",
    "    num_workers=2,         # 高速化：並列データ読み込み数 2が一番早いとか？\n",
    "    pin_memory=True,       # 高速化：GPU 転送を最適化\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e3d65",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-1-a. データセット定義の完成例\n",
    "\n",
    "訓練用データセット `train_ds`, 検証用データセット `val_ds`\n",
    "\n",
    "それぞれに対応する`DataLoader`である，`train_dl`, `val_dl` を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5070, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]) # CIFAR100用\n",
    "    ])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5070, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2761]) # CIFAR100用\n",
    "    ])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root=\"path/to/datasets\", train=True, transform=train_tf, download=True)\n",
    "val_ds = torchvision.datasets.CIFAR10(root=\"path/to/datasets\", train=False, transform=val_tf)\n",
    "\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf204d",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-2. Model の定義\n",
    "\n",
    "パターンは様々だが，だいたい次の3つ\n",
    "1. 公式からimport\n",
    "2. 外部からもってくる\n",
    "3. 自作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c0302",
   "metadata": {},
   "source": [
    "1. 公式からimport\n",
    "\n",
    "    - PyTorch の torchvision.models には、代表的なモデル（ResNet、VGG、AlexNet、MobileNet など） が用意されており、簡単に利用できる。\n",
    "\n",
    "    - pretrained=True で、ImageNetで事前学習された重みをロードできる。\n",
    "\n",
    "    - クラス数を指定する引数を必ずいれること！！入れないとImageNet用の1000クラス分類されちゃう．\n",
    "        - だいたい `num_classes` で指定できる．\n",
    "        - モデルの最後の全結合層の出力チャネル数を変更してる -> クラス数と同じ数の出力チャネルとなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import の例\n",
    "from torchvision import models\n",
    "model = models.resnet18(num_classes=10)  # 10クラス分類用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d6394",
   "metadata": {},
   "source": [
    "2. 外部からもってくる\n",
    "\n",
    "    - GitHubリポジトリなどからも実装を持ってこれる\n",
    "\n",
    "    - PyTorchに実装されていない新しめのモデルやカスタム実装が必要な時によく使う（例：CIFAR系のような，小さい画像を分類するためのアーキテクチャ）\n",
    "\n",
    "    - だいたい `models/` に `xxx.py` がいっぱい入ってる．そこをあさればお目当てのが見つかりやすいかも\n",
    "\n",
    "    - 環境に`.py`配置して `import` するとか，ターミナルからコマンドたたくとか，いろいろ実行方法あり\n",
    "\n",
    "    Ex.) CIFAR向けのアーキテクチャ：https://github.com/weiaicunzai/pytorch-cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd557d9",
   "metadata": {},
   "source": [
    "3. 自作\n",
    "\n",
    "    - `nn.Module`を継承する．\n",
    "\n",
    "    - `foward` は必ずオーバーライド．バッチ次元が含まれたテンソルに対して処理を書く必要がある．\n",
    "\n",
    "    - `__init__` に使う層を書いて，`foward` に処理の過程を書くのがセオリー\n",
    "    \n",
    "    詳細はPyTorch Tutorials に任せる．すこし調べて簡単なCNNが書けるくらいなら十分な気がする．\n",
    "\n",
    "    https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c6575",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-3. Loss の定義\n",
    "この2つ以外はほぼ使われないとおもわれる．\n",
    "\n",
    "- 分類タスク (離散値を予測) -> CrossEntoropyLoss\n",
    "\n",
    "- 回帰タスク (連続値を予測) -> MSELoss (平均二乗誤差)\n",
    "\n",
    "    注意：CrossEntoropyLossはモデルの出力(logit)を直接受け取ることを前提としている．\\\n",
    "    SoftmaxをとおしてからCrossEntoropyLossをとおすと正しい損失が算出できなくなる．\\\n",
    "    すでにCrossEntoropyLossは内部にSoftmaxの処理がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20204c58",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-4. Optimizer の定義\n",
    "第一引数に更新対象のパラメータのジェネレータを指定する．例示した通りに指定すればok\n",
    "\n",
    "さまざまな Optimizer があるが，研究でよく使われるのは次のとおり：\n",
    "\n",
    "CNN系：momentum SGD, Adam, AdamW\\\n",
    "Transformer系：AdamW\n",
    "\n",
    "- SGD (w/ momentum/Nesterov)\\\n",
    "汎化性能・安定性・計算コストの面で最強．だが，収束がかなり遅い\n",
    "\n",
    "- Adam\\\n",
    "非常に速く収束する．だが，汎化性能は微妙．学習安定性も高くない\n",
    "\n",
    "- AdamW\\\n",
    "Adamに重み減衰加えたやつ．Adamよりも汎化性能・安定性が高い．かなり人気らしい．transformer系は特に\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e47e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True) # よく使われる設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6671514",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-5. Scheduler の定義\n",
    "Optimizer の学習率を上書きして変化させる．使わなくても訓練できるため使用必須ではない．\\\n",
    "だが，使うとだいたい精度上がるし，研究でも採用されることが多い．\n",
    "\n",
    "とりあえずCosineAnneling (+ WarmUp ?) を使えば問題ない．\\\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html\\\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "\n",
    "SGDとは相性バッチリで，Adam, AdamWとも合わんわけではないらしい．\n",
    "\n",
    "注意：基本的に epoch ごとに学習率を変化させるが，たまに iteration ごとに変化させるのを前提としたものがある．この場合，scheduler.step()を呼ぶ位置を変える必要がある．Ex.) OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d181a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6499349",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-6. device の定義\n",
    "PyTorchでは，GPUを活用してモデルの訓練を行うことが可能である．ただし，演算に使用するすべてのテンソルは，同じデバイス（CPUやGPUなど）上に配置されている必要がある．\n",
    "\n",
    "そのため，あらかじめ使用するデバイスを変数として定義しておき，テンソルの配置先を統一しておくと便利である．\n",
    "\n",
    "なお，`.to(device)` でテンソルのデバイスを合わせなければいけないのは，\"入力データ\" と \"モデル\" である．\n",
    "\n",
    "以下に一般的な記述例を示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # GPU があれば使う"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa13d22",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1-a. 定義の完成例\n",
    "モデル作成に必要なパーツを定義する例を示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "epochs = 100\n",
    "num_classes = 10\n",
    "lr = 0.005\n",
    "batch_size = 128\n",
    "\n",
    "train_tf = transforms.Compose([]) # 省略\n",
    "val_tf = transforms.Compose([]) # 省略\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root=\"path/to/datasets\", train=True, transform=train_tf, download=True)\n",
    "val_ds = torchvision.datasets.CIFAR10(root=\"path/to/datasets\", train=False, transform=val_tf)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f361ad",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2-1. 訓練フローの定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb863d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練フローを定義\n",
    "def train_1epoch(model, train_dl, criterion, optimizer, scheduler=None, device=\"cpu\"):\n",
    "    total_loss = 0.0\n",
    "    total_corr = 0\n",
    "\n",
    "    model.train()                                       # 訓練モードに設定\n",
    "    for inputs, labels in train_dl:                     # ミニバッチを取得してループ\n",
    "        inputs = inputs.to(device)                      # GPU に転送\n",
    "        labels = labels.to(device)                      # GPU に転送\n",
    "\n",
    "        outputs = model(inputs)                         # 順伝播\n",
    "\n",
    "        loss = criterion(outputs, labels)               # 損失計算\n",
    "        preds = torch.argmax(outputs.detach(), dim=1)   # 予測値を取得\n",
    "        corr = torch.sum(preds == labels.data).item()   # 正解数をカウント\n",
    "\n",
    "        optimizer.zero_grad()                           # 勾配を初期化\n",
    "        loss.backward()                                 # 誤差逆伝播\n",
    "        optimizer.step()                                # 重み更新\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)         # 損失を累積\n",
    "        total_corr += corr                              # 正解数を累積\n",
    "\n",
    "    if scheduler is not None:                           # 学習率スケジューラが指定されている場合\n",
    "        scheduler.step()                                # 学習率を更新\n",
    "    train_loss = total_loss / len(train_dl.dataset)     # 平均損失を計算\n",
    "    train_acc = total_corr / len(train_dl.dataset)      # 平均精度を計算\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56478649",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2-2. 検証フローの定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea216d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証フローを定義\n",
    "def val_1epoch(model, val_dl, criterion, device=\"cpu\"):\n",
    "    total_loss = 0.0\n",
    "    total_corr = 0\n",
    "\n",
    "    model.eval()                                            # 評価モードに設定\n",
    "    with torch.no_grad():                                   # 勾配計算を無効化\n",
    "        for inputs, labels in val_dl:                       # ミニバッチを取得してループ\n",
    "            inputs = inputs.to(device)                      # GPU に転送\n",
    "            labels = labels.to(device)                      # GPU に転送\n",
    "            \n",
    "            outputs = model(inputs)                         # 順伝播\n",
    "\n",
    "            loss = criterion(outputs, labels)               # 損失計算\n",
    "            preds = torch.argmax(outputs.detach(), dim=1)   # 予測値を取得\n",
    "            corr = torch.sum(preds == labels.data).item()   # 正解数をカウント\n",
    "\n",
    "            total_loss += loss.item() * len(inputs)         # 損失を累積\n",
    "            total_corr += corr                              # 正解数を累積\n",
    "\n",
    "    val_loss = total_loss / len(val_dl.dataset)             # 平均損失を計算\n",
    "    val_acc = total_corr / len(val_dl.dataset)              # 平均精度を計算\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ad823",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2-3. 訓練\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "model = model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_1epoch(model, train_dl, criterion, optimizer, scheduler, device)\n",
    "    val_loss, val_acc = val_1epoch(model, val_dl, criterion, device)\n",
    "    print(f\"epoch {epoch+1:>3}/{epochs:>3}: train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8867e",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3-1. モデルの保存\n",
    "一般的に，`model`の`state_dict`と呼ばれるものを保存するのが一般的である．\n",
    "\n",
    "state_dict を保存しておけば，モデルのパラメータやバッファが保存され，同じ状態を後で再現できるようになる．\n",
    "\n",
    "モデルをロードする際，形状が合わないとエラーが発生して読み込めない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a88551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"tmp.pth\") # モデルの重みを保存\n",
    "model.load_state_dict(torch.load(\"tmp.pth\")) # モデルの重みを読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461cfd9",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3-2. 実験管理\n",
    "研究を始めると，様々な条件やハイパーパラメータを調整して実験する．\\\n",
    "実験結果や，学習環境を覚えておくことは無理．\\\n",
    "そこで，うまく結果や学習条件を記録する仕組みが必要．\n",
    "\n",
    "- ターミナルへの出力：これで何度も失敗してきた\n",
    "\n",
    "- 人力メモ帳ログ：めんどい 条件さぼって結局忘れる．再実験\n",
    "\n",
    "-> ツールに頼って！\n",
    "\n",
    "- Weight & Biases：一番トレンド\\\n",
    "https://wandb.ai/site/ja/\n",
    "\n",
    "- mlflow：手軽で使いやすい\\\n",
    "https://mlflow.org/\n",
    "\n",
    "- 自作：融通利くから超便利だけどコーディングめんどい\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daiki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
