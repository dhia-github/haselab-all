{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e30a3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\haselab-all\\PyTorch_practice\\wandb\\run-20250519_164530-q8w3uawh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh' target=\"_blank\">experiment-20250519-164530</a></strong> to <a href='https://wandb.ai/fukui-university/haselab-conpetition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fukui-university/haselab-conpetition' target=\"_blank\">https://wandb.ai/fukui-university/haselab-conpetition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh' target=\"_blank\">https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x20d9f430d60>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb.init(project=\"haselab-conpetition\", name=f\"experiment-{timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4043a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "633ea1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットをダウンロード・展開するコード\n",
    "# 学外にいる場合や，Google Colaboratory で実行する場合，このコードはうまく動かない．\n",
    "# データセットをアクセスできる場所に置き，dataset_dir にデータセットのパスを指定する必要がある．\n",
    "\n",
    "# 具体的手順は以下\n",
    "# Google Colaboratory で実行する場合\n",
    "# 1. Aの部分をコメントアウト\n",
    "# 2. zipファイルをアップロードする．\n",
    "# 3. Bの部分をアンコメント\n",
    "\n",
    "# それ以外\n",
    "# 1. competition_images/ をアクセスできる場所に置く\n",
    "# 2. Aの部分をコメントアウト\n",
    "# 3. Cをアンコメントし，competitions_images/ のパスとなる文字列を格納  Ex.) dataset_dir = \"/home/haselab/Documents/tat/tmp/competition_images\"\n",
    "\n",
    "\n",
    "\n",
    "#      ----- A -----\n",
    "#      # データセットの保存先を指定\n",
    "#      save_dir = str(Path().resolve()) # 保存ディレクトリをノートブックと同じディレクトリに設定\n",
    "#      dataset_dir = save_dir + \"/competition_images/\"\n",
    "#      # データセットを保存\n",
    "#      if Path(dataset_dir).exists():\n",
    "#          print(f\"Dataset directory already exists: {dataset_dir}\")\n",
    "#      else:\n",
    "#          zip_path = Path(str(save_dir)) / \"tmp.zip\"\n",
    "#          url = \"http://10.0.87.42:8080/dataset\" # こっちでもいける\n",
    "#          subprocess.run([\"wget\", url, \"-O\", zip_path, \"--no-proxy\", \"-q\"], check=True) # .pyならこっち\n",
    "#          # !wget -O {zip_path} {url} -q --no-proxy\n",
    "#          with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#              zip_ref.extractall(save_dir)\n",
    "#              if zip_path.exists():\n",
    "#                  zip_path.unlink()\n",
    "#          print(f\"Downloaded to {save_dir} and extracted.\")\n",
    "#      ----- A -----\n",
    "\n",
    "\n",
    "\n",
    "# ----- B -----\n",
    "# zip_path = \"/content/competition_images.zip\" # zipファイルのパスを指定\n",
    "# ext_dir = \"/content/\"  # 展開先を指定\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(ext_dir)\n",
    "# dataset_dir = ext_dir + \"competition_images/\" # データセットのパスを指定\n",
    "# ----- B -----\n",
    "\n",
    "\n",
    "# ----- C -----\n",
    "dataset_dir = \"./content/competition_images\" # 要変更．データセットのパスを指定\n",
    "# ----- C -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0f3c8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータ読み込みのためのカスタムデータセットクラス\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, target_transform=None):\n",
    "        self.img_paths = sorted([p for p in Path(path).iterdir()])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_paths[index]\n",
    "        data = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform: # 画像の前処理\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            path = self.target_transform(path) # 画像のパスに前処理している。名前を変更？\n",
    "        return data, str(path.name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a507e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータを指定\n",
    "epochs = 12\n",
    "lr = 0.0001\n",
    "batch_size = 256\n",
    "\n",
    "wandb.config.update({\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4733d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, full_dataset, indices, transform=None):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        image, label = self.full_dataset[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "53edc45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device : GPU\n"
     ]
    }
   ],
   "source": [
    "# モデルと訓練に必要なものを定義\n",
    "num_classes = 4\n",
    "\n",
    "train_tf = torchvision.transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0)), #80%~100%の範囲で切り取る\n",
    "    transforms.RandomHorizontalFlip(), #?ランダムに反転->かえって精度が落ちた。\n",
    "    transforms.RandomRotation(15), #?ランダムに回転\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), #?色の変化\n",
    "    \n",
    "    transforms.ToTensor(),                                        #?追加で反転とか\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_tf = torchvision.transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #?平均と重みを事前訓練時の値に変更\n",
    "])\n",
    "\n",
    "test_tf = torchvision.transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #?平均と重みを事前訓練時の値に変更\n",
    "])\n",
    "\n",
    "\n",
    "#分割--------------------------------------------------------------------------------\n",
    "temp_ds = ImageFolder(root=dataset_dir + \"/train_val\")\n",
    "dataset_size = len(temp_ds)\n",
    "val_split = 0.2\n",
    "val_count = int(dataset_size * val_split)\n",
    "train_count = dataset_size - val_count\n",
    "\n",
    "generator = torch.Generator().manual_seed(89) #? 乱数シードを固定\n",
    "train_subset_indices, val_subset_indices = random_split(range(dataset_size), [train_count, val_count], generator = generator)\n",
    "\n",
    "train_ds = CustomDataset(temp_ds, train_subset_indices, transform=train_tf) #? train_tfを適用\n",
    "val_ds = CustomDataset(temp_ds, val_subset_indices, transform=val_tf) #? val_tfを適用\n",
    "\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "#train_ds = ImageFolder(root=dataset_dir + \"/train_val\", transform=train_tf)\n",
    "test_ds = TestDataset(path=dataset_dir + \"/test\", transform=test_tf)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# model = torchvision.models.resnet18(num_classes=num_classes) # 事前学習済みモデルを使用しない重みはランダム初期化\n",
    "\n",
    "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT) # 事前学習済みモデルを使用する。ファインチューニング\n",
    "num_ftrs = model.fc.in_features # 元の全結合層の入力特徴数を入手\n",
    "\n",
    "#ドロップアウト層を追加\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, num_classes) # 全結合層を新しく定義\n",
    ")\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes) # 全結合層を新しく定義\n",
    "\n",
    "\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False # まず全ての層の勾配計算をオフ\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True # fc層のみ勾配計算をオン\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4) #? \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) #? 学習率を0.9倍にするスケジューラ\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device : \" + ( \"GPU\" if device.type == \"cuda\" else \"CPU\" )) #どっちつかってるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bb863d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練フローを定義\n",
    "def train_1epoch(model, train_dl, criterion, optimizer, scheduler=None, device=\"cpu\"):\n",
    "    total_loss = 0.0\n",
    "    total_corr = 0\n",
    "\n",
    "    model.train()                                       # 訓練モードに設定\n",
    "    for inputs, labels in train_dl:                     # ミニバッチを取得してループ\n",
    "        inputs = inputs.to(device)                      # GPU に転送\n",
    "        labels = labels.to(device)                      # GPU に転送\n",
    "\n",
    "        outputs = model(inputs)                         # 順伝播\n",
    "\n",
    "        loss = criterion(outputs, labels)               # 損失計算\n",
    "        preds = torch.argmax(outputs.detach(), dim=1)   # 予測値を取得 各データの予測の最大値のインデックスを収集する操作なので、別に追跡する必要はないってかしたら勾配計算が異常になるのでdetach()。\n",
    "        corr = torch.sum(preds == labels.data).item()   # 正解数をカウント\n",
    "\n",
    "        optimizer.zero_grad()                           # 勾配を初期化\n",
    "        loss.backward()                                 # 誤差逆伝播\n",
    "        optimizer.step()                                # 重み更新\n",
    "\n",
    "        total_loss += loss.item() * len(inputs)         # 損失を累積\n",
    "        total_corr += corr                              # 正解数を累積\n",
    "\n",
    "    # if scheduler is not None:                           # 学習率スケジューラが指定されている場合\n",
    "    #     scheduler.step()                                # 学習率を更新\n",
    "    train_loss = total_loss / len(train_dl.dataset)     # 平均損失を計算\n",
    "    train_acc = total_corr / len(train_dl.dataset)      # 平均精度を計算\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "af2dfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_1epoch(model, val_dl, criterion, device=\"cpu\"):\n",
    "    total_loss = 0.0\n",
    "    total_corr = 0\n",
    "\n",
    "    model.eval()                                        # 評価モードに設定\n",
    "    with torch.no_grad():                               # 勾配計算を無効化\n",
    "        for inputs, labels in val_dl:                   # ミニバッチを取得してループ\n",
    "            inputs = inputs.to(device)                  # GPU に転送\n",
    "            labels = labels.to(device)                  # GPU に転送\n",
    "\n",
    "            outputs = model(inputs)                     # 順伝播\n",
    "\n",
    "            loss = criterion(outputs, labels)           # 損失計算\n",
    "            preds = torch.argmax(outputs, dim=1)        # 予測値を取得\n",
    "            corr = torch.sum(preds == labels.data).item()    # 正解数をカウント\n",
    "\n",
    "            total_loss += loss.item() * len(inputs)     # 損失を累積\n",
    "            total_corr += corr                          # 正解数を累積\n",
    "\n",
    "    val_loss = total_loss / len(val_dl.dataset)         # 平均損失を計算\n",
    "    val_acc = total_corr / len(val_dl.dataset)          # 平均精度を計算\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2e631371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 推論フローを定義\n",
    "def pred(model, test_dl, device, probs=False, categorize=False):\n",
    "    total_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"推論ループが開始された\")\n",
    "        loop_start_time = time.time() #ループ開始時刻\n",
    "\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(test_dl):\n",
    "            print(f\"Processing batch {i+1}/{len(test_dl)}...\")\n",
    "            batch_start_time = time.time() #バッチ開始時刻\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if probs:\n",
    "                outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            if categorize:\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            total_outputs.append(outputs)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "            batch_end_time = time.time() #バッチ終了時刻\n",
    "            if (i + 1) % 10 == 0: # 10バッチごとに進捗を表示\n",
    "                print(f\"Processed batch {i+1}/{len(test_dl)}, time per batch: {batch_end_time - batch_start_time:.4f}s\")\n",
    "\n",
    "        loop_end_time = time.time()\n",
    "        print(f\"Prediction loop finished in {loop_end_time - loop_start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    outputs = torch.cat(total_outputs, dim=0).cpu().tolist()\n",
    "    return outputs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4ab3e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\furaf\\anaconda3\\envs\\torch-env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9c69e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 12: train_loss: 1.2650, train_acc: 0.4458\n",
      "epoch   1/ 12: val_loss: 1.7727, val_acc: 0.2625\n",
      "epoch   2/ 12: train_loss: 0.7178, train_acc: 0.7260\n",
      "epoch   2/ 12: val_loss: 1.1925, val_acc: 0.4167\n",
      "epoch   3/ 12: train_loss: 0.5939, train_acc: 0.7927\n",
      "epoch   3/ 12: val_loss: 0.9338, val_acc: 0.5833\n",
      "epoch   4/ 12: train_loss: 0.4865, train_acc: 0.8292\n",
      "epoch   4/ 12: val_loss: 0.8891, val_acc: 0.6417\n",
      "epoch   5/ 12: train_loss: 0.4361, train_acc: 0.8500\n",
      "epoch   5/ 12: val_loss: 0.8586, val_acc: 0.6750\n",
      "epoch   6/ 12: train_loss: 0.4377, train_acc: 0.8448\n",
      "epoch   6/ 12: val_loss: 0.9167, val_acc: 0.6708\n",
      "epoch   7/ 12: train_loss: 0.4077, train_acc: 0.8448\n",
      "epoch   7/ 12: val_loss: 0.9482, val_acc: 0.6667\n",
      "epoch   8/ 12: train_loss: 0.3904, train_acc: 0.8698\n",
      "epoch   8/ 12: val_loss: 1.0637, val_acc: 0.6542\n",
      "epoch   9/ 12: train_loss: 0.3743, train_acc: 0.8646\n",
      "epoch   9/ 12: val_loss: 1.1058, val_acc: 0.6583\n",
      "epoch  10/ 12: train_loss: 0.3675, train_acc: 0.8646\n",
      "epoch  10/ 12: val_loss: 1.1732, val_acc: 0.6583\n",
      "epoch  11/ 12: train_loss: 0.3575, train_acc: 0.8729\n",
      "epoch  11/ 12: val_loss: 1.1789, val_acc: 0.6542\n",
      "epoch  12/ 12: train_loss: 0.3076, train_acc: 0.8812\n",
      "epoch  12/ 12: val_loss: 1.1445, val_acc: 0.6750\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇████████</td></tr><tr><td>val_loss</td><td>█▄▂▁▁▁▂▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.88125</td></tr><tr><td>train_loss</td><td>0.30757</td></tr><tr><td>val_accuracy</td><td>0.675</td></tr><tr><td>val_loss</td><td>1.14447</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment-20250519-164530</strong> at: <a href='https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh' target=\"_blank\">https://wandb.ai/fukui-university/haselab-conpetition/runs/q8w3uawh</a><br> View project at: <a href='https://wandb.ai/fukui-university/haselab-conpetition' target=\"_blank\">https://wandb.ai/fukui-university/haselab-conpetition</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250519_164530-q8w3uawh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練\n",
    "model = model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_1epoch(model, train_dl, criterion, optimizer, scheduler, device=device)\n",
    "    print(f\"epoch {epoch+1:>3}/{epochs:>3}: train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}\")\n",
    "\n",
    "    val_loss, val_acc = validate_1epoch(model, val_dl, criterion, device=device)\n",
    "    print(f\"epoch {epoch+1:>3}/{epochs:>3}: val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if scheduler is not None:                           # 学習率スケジューラが指定されている場合\n",
    "        scheduler.step(val_loss)                                # 学習率を更新\n",
    "\n",
    "    wandb.log({\"val_accuracy\": val_acc, \"val_loss\": val_loss}) # wandb にログを記録\n",
    "    wandb.log({\"train_accuracy\": train_acc, \"train_loss\": train_loss}) # wandb にログを記録\n",
    "\n",
    "wandb.save(f\"model_epoch{epoch+1}.pth\") # wandb にモデルを保存\n",
    "wandb.finish() # wandb のセッションを終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8aa41216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推論ループが開始された\n",
      "Processing batch 1/9...\n",
      "Processing batch 2/9...\n",
      "Processing batch 3/9...\n",
      "Processing batch 4/9...\n",
      "Processing batch 5/9...\n",
      "Processing batch 6/9...\n",
      "Processing batch 7/9...\n",
      "Processing batch 8/9...\n",
      "Processing batch 9/9...\n",
      "Prediction loop finished in 0.85 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 推論・結果をCSV形式で保存\n",
    "\n",
    "csv_name = f\"result/predictions{timestamp}.csv\"\n",
    "\n",
    "outputs, all_labels = pred(model, test_dl, device, categorize=True)\n",
    "lines = [f\"{l},{o}\" for o, l in zip(outputs, all_labels)]\n",
    "csv_text = \"\\n\".join(lines)\n",
    "\n",
    "with open(csv_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aa095",
   "metadata": {},
   "source": [
    "変更点\n",
    "モデルのresnet18を使用\n",
    "事前に訓練された重みを利用＋最後の結合層をカスタマイズ（ファインチューニング）\n",
    "ハイパラは以下のように\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "オプティマイザをAdamWに変更\n",
    "スケジューラの追加"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
