. 手法
3.1 異常検知のための特徴再構成の再検討
図2では、特徴再構成パラダイム に従い、MLP、CNN、およびTransformer（クエリ埋め込みあり）を構築し、事前学習済みバックボーンによって抽出された特徴を再構成します 。再構成誤差が異常の可能性を表します 。3つのネットワークのアーキテクチャは付録に記載されています 。メトリックは10エポックごとに評価されます 。なお、学習中に異常は利用できないため、定期的な評価は非現実的です 。





図2aに示すように、一定期間の学習後、3つのネットワークの性能は損失が極端に小さくなるにつれて著しく低下します 。我々はこの原因を「

恒等ショートカット」問題にあると見ています。これは、正常領域と異常領域の両方がうまく復元されてしまい、結果として異常を発見できなくなる現象です 。この推測は、図2bの可視化結果によって検証されます（より多くの結果は付録にあります）。しかし、MLPやCNNと比較して、Transformerは性能の低下がはるかに小さく、ショートカット問題がより軽微であることを示唆しています 。これが我々の以下の分析を促しました 。




正常画像の特徴を 

x 
+
 ∈R 
K×C
  とし、Kは特徴数、Cはチャネル次元です 。簡単のためバッチ次元は省略します 。同様に、異常画像の特徴は 


x 
−
 ∈R 
K×C
  と表記します 。再構成損失にはMSE損失を選択します 。


再構成ネットとして単純な1層ネットワークを用いて大まかな分析を行います。このネットワークは 

x 
+
  で学習され、x 
−
  の異常領域を検出するためにテストされます 。


MLPの全結合層: この層の重みとバイアスをそれぞれ w∈R 
C×C
 、b∈R 
C
  とすると、この層は y=x 
+
 w+b∈R 
K×C
  と表現できます 。MSE損失がyを 

x 
+
  に近づけるように作用すると、モデルは w→I（単位行列）、b→0 となるショートカットを取る可能性があります 。最終的に、このモデルは 

x 
−
  も同様にうまく再構成してしまい、異常検知に失敗します 。


CNNの畳み込み層: l×l カーネルを持つ畳み込み層は全結合層と等価です 。また、

n×n(n>1) カーネルはより多くのパラメータと大きな容量を持ち、1×1 カーネルができることは何でもできます 。したがって、この層もショートカットを学習する可能性があります 。



クエリ埋め込みを持つTransformer: このようなモデルでは、学習可能なクエリ埋め込み q∈R 
K×C
  を持つアテンション層が存在します 。この層を再構成モデルとして使用する場合、次のように表されます：

y=softmax(q(x 
+
 ) 
T
 / 
C

​
 )x 
+
 ∈R 
K×C
  。yを 

x 
+
  に近づけるためには、アテンションマップ softmax(q(x 
+
 ) 
T
 / 
C

​
 ) が単位行列Iに近づく必要があり、そのためにはqが x 
+
  と高い関連性を持つ必要があります 。学習済みモデルのqが正常サンプルと関連していることを考慮すると、このモデルは 

x 
−
  をうまく再構成することはできません 。セクション4.6の切除実験では、クエリ埋め込みがない場合、Transformerの性能は異常検知で18.1%、位置特定で13.4%と劇的に低下することが示されています 。したがって、クエリ埋め込みは正常分布をモデル化する上で極めて重要です 。



しかし、Transformerも依然としてショートカット問題に悩まされており、これが我々の3つの改善策を着想させました 。

クエリ埋め込みが異常の再構成を防ぐことができるという点に基づき、我々はバニラTransformerの最初の層だけでなく、各デコーダ層にクエリ埋め込みを追加する**レイヤーワイズクエリデコーダ（LQD）**を設計しました 。

我々は、完全なアテンションがショートカットの可能性を高めると疑っています 。あるトークンが自身とその近傍領域を見ることができるため、単純なコピーによって再構成が容易になります 。そこで、アテンションマップを計算する際に近傍トークンをマスクする**近傍マスク付きアテンション（NMA）**を提案します 。



我々は、入力特徴を乱す**特徴ジッタリング（FJ）**戦略を採用し、モデルがノイズ除去から正常な分布を学習するように導きます 。

これらの設計のおかげで、我々のUniADは図2に示すように満足のいく性能を達成します 。

「恒等ショートカット」問題と統一ケースの関係
図2aでは、「恒等ショートカット」問題を可視化することを目的としており、そこでは損失が小さくなるにつれて性能が低下します 。我々は同じ実験をMLPの個別ケースで実施しました 。図4に示すように、個別ケースでは損失（青）が小さくなるにつれて精度（緑：検知、赤：位置特定）は上昇し続けます 。これは、「恒等ショートカット」問題と統一ケースの関係を明らかにするのに役立ちます。すなわち、統一ケースはより挑戦的であるため、「恒等ショートカット」問題を増幅させるのです 。したがって、我々のアプローチは「恒等ショートカット」問題を解決するために特別に設計されているため、統一ケースで効果的であると言えます 。





3.2 統一的異常検知のための特徴再構成の改善

概要: 図3に示すように、我々のUniADは**近傍マスク付きエンコーダ（NME）とレイヤーワイズクエリデコーダ（LQD）で構成されます 。まず、固定された事前学習済みバックボーンによって抽出された特徴トークンは、NMEによってさらに統合され、エンコーダ埋め込みが導出されます 。次に、LQDの各層で、学習可能なクエリ埋め込みがエンコーダ埋め込みおよび前の層の出力と順次融合されます（最初の層では自己融合）。特徴融合は



近傍マスク付きアテンション（NMA）によって行われます 。LQDの最終出力が再構成された特徴と見なされます 。また、我々は入力特徴に摂動を加える


特徴ジッタリング（FJ）**戦略を提案し、モデルがノイズ除去タスクから正常分布を学習するように導きます 。最終的に、異常の位置特定と検知の結果は、再構成の差分を通じて得られます 。



近傍マスク付きアテンション（NMA）: バニラTransformerの完全なアテンションが「恒等ショートカット」に寄与していると我々は疑っています 。完全なアテンションでは、1つのトークンが自身を見ることが許されるため、単純なコピーによる再構成が容易になります 。さらに、特徴トークンがCNNバックボーンによって抽出されることを考慮すると、近傍トークンは多くの類似性を共有しているはずです 。したがって、我々はアテンションマップを計算する際に近傍トークンをマスクすることを提案し、これを近傍マスク付きアテンション（NMA）と呼びます 。近傍領域は図5に示すように2D空間で定義されることに注意してください 。






近傍マスク付きエンコーダ（NME）: エンコーダはバニラTransformerの標準的なアーキテクチャに従います 。各層はアテンションモジュールとフィードフォワードネットワーク（FFN）で構成されます 。ただし、完全なアテンションは情報漏洩を防ぐために我々が提案するNMAに置き換えられます 。




レイヤーワイズクエリデコーダ（LQD）: セクション3.1で分析したように、クエリ埋め込みは異常をうまく再構成するのを防ぐのに役立ちます 。しかし、バニラTransformerにはクエリ埋め込みが1つしかありません 。そこで、我々は図3に示すように、クエリ埋め込みの使用を強化するためにレイヤーワイズクエリデコーダ（LQD）を設計しました 。具体的には、LQDの各層で、学習可能なクエリ埋め込みがまずエンコーダ埋め込みと融合され、次に前の層の出力と統合されます（最初の層では自己統合）。特徴融合はNMAによって実装されます 。バニラTransformerに倣い、2層のFFNがこれらの融合されたトークンを処理するために適用され、学習を容易にするために残差接続が利用されます 。LQDの最終出力が再構成された特徴として機能します 。






特徴ジッタリング（FJ）: ノイズ除去オートエンコーダ（DAE）に着想を得て、我々は特徴トークンに摂動を加え、モデルがノイズ除去タスクによって正常サンプルの知識を学習するように導きます 。具体的には、特徴トークン 

f 
tok
​
 ∈R 
C
  に対して、ガウス分布から摂動Dをサンプリングします：D∼N(μ=0,σ 
2
 =(α 
C
∣∣f 
tok
​
 ∣∣ 
2
​
 
​
 ) 
2
 ) 。ここで、αはノイズの度合いを制御するジッタリングスケールです 。また、サンプリングされた摂動は、固定されたジッタリング確率pで 


f 
tok
​
  に加えられます 。

3.3 実装詳細

特徴抽出: 我々は、ImageNetで事前学習済みの固定されたEfficientNet-b4を特徴抽出器として採用します 。ステージ1からステージ4までの特徴が選択されます 。ここでのステージとは、同じサイズの特徴マップを持つブロックの組み合わせを意味します 。これらの特徴は同じサイズにリサイズされ、チャネル次元に沿って連結され、特徴マップ 


f 
org
​
 ∈R 
C 
org
​
 ×H×W
  を形成します 。


特徴再構成: 特徴マップ f 
org
​
  は、まず H×W 個の特徴トークンに分割され、続いて線形射影によって C 
org
​
  がより小さなチャネルCに削減されます 。その後、これらのトークンはNMEとLQDによって処理されます 。学習可能な位置埋め込みが、空間情報を伝えるためにアテンションモジュールに追加されます 。その後、別の線形射影がチャネルをCから 


C 
org
​
  に戻すために使用されます 。再形成後、再構成された特徴マップ 

f 
rec
​
 ∈R 
C 
org
​
 ×H×W
  が最終的に得られます 。


目的関数: 我々のモデルはMSE損失で学習されます：L= 
H×W
1
​
 ∣∣f 
org
​
 −f 
rec
​
 ∣∣ 
2
2
​
  。


異常位置特定の推論: 異常位置特定の結果は、各ピクセルに異常スコアを割り当てる異常スコアマップです 。具体的には、異常スコアマップsは、再構成差分のL2ノルムとして計算されます：

s=∣∣f 
org
​
 −f 
rec
​
 ∣∣ 
2
​
 ∈R 
H×W
  。その後、sはバイリニア補間によって画像サイズにアップサンプリングされ、位置特定結果が得られます 。



異常検知の推論: 異常検知は、画像に異常領域が含まれているかどうかを検出することを目的とします 。我々は、平均プーリングされたsの最大値を取ることで、異常スコアマップsを画像の異常スコアに変換します 。